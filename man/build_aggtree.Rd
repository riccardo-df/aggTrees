% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/aggregation-trees.R
\name{build_aggtree}
\alias{build_aggtree}
\alias{inference_aggtree}
\title{Aggregation Trees}
\usage{
build_aggtree(
  y,
  D,
  X,
  honest_frac = 0.5,
  method = "aipw",
  scores = NULL,
  cates = NULL,
  is_honest = NULL,
  ...
)

inference_aggtree(object, n_groups)
}
\arguments{
\item{y}{Outcome vector.}

\item{D}{Treatment vector.}

\item{X}{Covariate matrix (no intercept).}

\item{honest_frac}{Fraction of observations to be allocated to honest sample.}

\item{method}{Either \code{"raw"} or \code{"aipw"}, controls how node predictions are computed.}

\item{scores}{Optional, vector of scores to be used in computing node predictions. Useful to save computational time if scores have already been estimated. Ignored if \code{method == "raw"}.}

\item{cates}{Optional, estimated CATEs. If not provided by the user, CATEs are estimated internally via a \code{\link[grf]{causal_forest}}.}

\item{is_honest}{Logical vector denoting which observations belong to the honest sample. Required only if the \code{cates} argument is used.}

\item{...}{Further arguments from \code{\link[rpart]{rpart.control}}.}

\item{object}{An \code{aggTrees} object.}

\item{n_groups}{Number of desired groups.}
}
\value{
\code{\link{build_aggtree}} returns an \code{aggTrees} object.\cr

\code{\link{inference_aggtree}} returns an \code{aggTrees.inference} object, which in turn contains the \code{aggTrees} object used
in the call.
}
\description{
Nonparametric data-driven approach to discovering heterogeneous subgroups in a selection-on-observables framework.
The approach constructs a sequence of groupings, one for each level of granularity. Groupings are nested and
feature an optimality property. For each grouping, we obtain point estimation and standard errors for group average
treatment effects (GATEs). Additionally, we assess whether systematic heterogeneity is found by testing the hypothesis
that GATEs are the same across groups and that each GATE is different from the smallest. Finally, we investigate the driving
factors of effect heterogeneity by computing the average characteristics of units in each group.
}
\details{
Aggregation trees are a three-step procedure. First, conditional average treatment effects (CATEs) are estimated using any
estimator. Second, a tree is grown to approximate the CATEs. Third, the tree is pruned to derive a nested sequence of optimal
groupings, one for each granularity level. For each level of granularity, we can obtain point estimation and inference about
GATEs.\cr

To implement this methodology, the user can rely on two core functions that handle the various steps.\cr
\subsection{Constructing the Sequence of Groupings}{

\code{\link{build_aggtree}} constructs the sequence of groupings (i.e., the tree) and estimate GATEs in each node. GATEs can be estimated
in several ways. This is controlled by the \code{method} argument. If \code{method == "raw"}, we compute the difference
in mean outcomes between treated and control observations in each node. This is an unbiased estimator in randomized experiment.
If \code{method == "aipw"}, we construct doubly-robust scores and average them in each node. This is unbiased also in
observational studies. Honest regression forests and 5-fold cross fitting are used to estimate the propensity score and the
conditional mean function of the outcome (unless the user specifies the argument \code{scores}).\cr

The user can provide a vector of estimated CATEs via the \code{cates} argument. If so, the user needs to specify a logical
vector to denote which observations belong to the honest sample. If honesty is not desired, \code{is_honest} must be a
vector of \code{FALSE}s. If no vector of CATEs is provided, these are estimated internally via a
\code{\link[grf]{causal_forest}}.\cr
}

\subsection{GATEs Estimation and Inference}{

\code{\link{inference_aggtree}} takes as input an \code{aggTrees} object constructed by \code{\link{build_aggtree}}. Then, for the
desired granularity level, chosen via the \code{n_groups} argument, it provides point estimation and standard errors for
GATEs. Additionally, it performs some hypothesis testing to assess whether we find systematic heterogeneity and computes
the average characteristics of the units in each group to investigate the driving mechanism.
\subsection{Point estimates and standard errors for GATEs}{

GATEs and their standard errors are obtained by fitting an appropriate linear model. If \code{method == "raw"}, we estimate
via OLS the following:

\deqn{Y_i = \sum_{l = 1}^{|T|} L_{i, l} \gamma_l + \sum_{l = 1}^{|T|} L_{i, l} D_i \beta_l + \epsilon_i}

with \code{L_{i, l}} a dummy variable equal to one if the i-th unit falls in the l-th group, and |T| the
number of groups. If the treatment is randomly assigned, one can show that the betas identify the GATE of
each group. However, this is not true in observational studies due to selection into treatment. In this case, the user is expected
to use \code{method == "aipw"} when calling \code{\link{build_aggtree}}. In this case, \code{\link{inference_aggtree}} uses the scores
in the following regression:

\deqn{score_i = \sum_{l = 1}^{|T|} L_{i, l} \beta_l + \epsilon_i}

This way, betas again identify GATEs. Regardless of \code{method}, standard errors are estimated via the Eicker-Huber-White
estimator.
}

\subsection{Hypothesis testing}{

\code{\link{inference_aggtree}} performs two types of hypothesis testing. First, it uses standard errors from the above models
to test whether GATEs in each leaf are the same by constructing a finite-sample F statistic for carrying out a Wald-test-based
comparison between a model and a linearly restricted model. Second, it fits a new linear model to estimate and make inference
about the difference between each GATE and the smallest GATE. If \code{method == "raw"}, the following model is used:

\deqn{Y_i = \delta + \lambda D_i + \sum_{l = 2}^{|T|} L_{i, l} \gamma_l + \sum_{l = 2}^{|T|} L_{i, l} D_i \beta_l + \epsilon_i}

where leaves 2, ..., |T| are ordered in increasing order of their GATEs. One can show that each beta_l identifies the difference
between the l-th GATE and the smallest GATE. Similarly, if \code{method == "aipw"} we fit the following model:

\deqn{score_i = \delta + \sum_{l = 2}^{|T|} L_{i, l} \beta_l + \epsilon_i}

to obtain the same result. We can then test separately the usual null hypotheses that each beta is zero. We adjust p-values to
account for multiple hypothesis testing using the Holm's procedure. As before, standard errors are always estimated via the
Eicker-Huber-White estimator.\cr
}

\subsection{Average Characteristics}{

\code{\link{inference_aggtree}} regresses each covariate on a set of dummies denoting group membership. This way, we get the
average characteristics of units in each leaf, together with a standard error. Leaves are ordered in increasing order of their
predictions (from most negative to most positive). Standard errors are estimated via the Eicker-Huber-White estimator.
}

}

\subsection{Caution on Inference}{

Regardless of the chosen \code{method}, both functions estimate GATEs, linear models, and average characteristics of units in each
group using only observations in the honest sample. If the honest sample is empty (this happens because the user either sets
\code{honest_frac = 0} or passes a vector of \code{FALSE}s as \code{is_honest} when calling \code{\link{build_aggtree}}), the
same data used to construct the tree are used to estimate the above quantities. This is fine for prediction but invalidates
inference.
}
}
\examples{
\donttest{
## Generate data.
set.seed(1986)

n <- 1000
k <- 3

X <- matrix(rnorm(n * k), ncol = k)
colnames(X) <- paste0("x", seq_len(k))
D <- rbinom(n, size = 1, prob = 0.5)
mu0 <- 0.5 * X[, 1]
mu1 <- 0.5 * X[, 1] + X[, 2]
y <- mu0 + D * (mu1 - mu0) + rnorm(n)

## Construct sequence of groupings. CATEs estimated internally,
groupings <- build_aggtree(y, D, X, method = "aipw")

## We can estimate CATEs and pass them.
splits <- sample_split(length(y), training_frac = 0.5)
training_idx <- splits$training_idx
honest_idx <- splits$honest_idx

y_tr <- y[training_idx]
D_tr <- D[training_idx]
X_tr <- X[training_idx, ]

y_hon <- y[honest_idx]
D_hon <- D[honest_idx]
X_hon <- X[honest_idx, ]

library(grf)
forest <- causal_forest(X_tr, y_tr, D_tr) # Use training sample.
cates <- predict(forest, X)$predictions

groupings <- build_aggtree(y, D, X, method = "aipw", cates = cates,
                           is_honest = 1:length(y) \%in\% honest_idx)

## We have compatibility with generic S3-methods.
summary(groupings)
print(groupings)
plot(groupings) # Try also setting 'sequence = TRUE'.

## To predict, do the following.
tree <- subtree(groupings$tree, cv = TRUE) # Select by cross-validation.
predict(tree, data.frame(X))

## Inference with 4 groups.
results <- inference_aggtree(groupings, n_groups = 4)

summary(results$model_gates) # Coefficient of leafk is GATE in k-th leaf.
results$gates_all_equal # We reject the null that all GATEs are equal.

summary(results$model_diff) # leafk is difference between k-th GATE and smallest GATE.
results$gates_diff_smallest # GATEs are significantly different from GATE in leaf 1.
print(results, table = "diff")

print(results, table = "avg_char")}

}
\references{
\itemize{
\item R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. \doi{10.2139/ssrn.4304256}.
}
}
\seealso{
\code{\link{plot.aggTrees}} \code{\link{print.aggTrees.inference}}
}
\author{
Riccardo Di Francesco
}
